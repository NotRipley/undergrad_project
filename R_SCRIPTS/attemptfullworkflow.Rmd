---
title: "Draft Workflow for Clustering Algorithm Comparison on scRNA-seq Data"
output: html_document
---

### Data to be considered

First in consideration are the two datasets we will consider. We will use the pbmc dataset and one chosen splatter sim for now. So in this section, we load, label, process etc etc the two datasets and get them both into seurat objects. We also sort out where our files are going to be stored, and before moving on we clean up our working directory a bit.

## Metadata to store as output

KMeans needs k (number of centres) to be estimated or varied. DBScan needs eps (max distance between points) and minpts (min pts for a cluster) the same. Louvain needs resolution to be estimated or varied.

So for each clustering attempt, we store:
- the algorithm info
  - the type
  - the parameters
- the dataset info
  - the name
  - the number of cells
  - the number of features
  - the number of PCs
  - (maybe here is whether we use UMAP or not?)
- the run info
  - which repetition this is
  - the random seed
  - the time at the start
  - the runtime 
- the cluster results
  - the number detected
  - the sizes of them
- the quality metrics
  - silhouette avg
  - silhouette sd - because silhouette is a per-node property and not initially a gobal one, it has some variance
  - the adj rand index
  - any more that we want - this can be considered an easy way to expand the scope i think
- then any biological context that we want to add

## How many repetitions

A lot of estimation for DBScan parameters involves computationally finding an 'elbow', which is fine but a bit clunky. AI suggests a grid search on the parameters and just storing all of them, but this seems really inefficient because I then just don't care about the ones that aren't optimal. 

But however we do it, we go with 30-50 repetitions by unique algorithm/parameter/dataset combination. And then we can extract the quality metrics from these 30-50 repetitions.

### Algorithms for Comparison

We want to compare KMeans, DBScan, and the Louvain Algorithm. If we expanded this scope, it would be to the other standard options in FindClusters(algorithm=...) from seurat.

### Quality Metrics

For our splatter data, we will have access to the ground truth. In this case, we can use the adjusted rand index for the cluster quality assessment. We also do the silhouette score. 


### We Begin

```{r Libraries}

suppressPackageStartupMessages({
  library(splatter)
  library(scater)
  library(Seurat)
  library(tidyverse)
})

```

```{r Functions}

# batchCells - a vector giving the number of cells per batch, ie two batches with 100 cells is (100, 100)
# group.prob - vector specifying probabilities per group, ie (0.5, 0.5) will likely give two same sized groups
# de.prob - probability that genes will be DE by group(?) i.e. that they'll differ more than just noise
# nGenes - number of genes to simulate
# all else we leave as default values. 
seurat_from_params <- function(batchCells, group.prob, de.prob, nGenes, proj_name) {
  
  stopifnot(
    is.numeric(batchCells),
    is.numeric(group.prob),
    is.numeric(de.prob),
    is.numeric(nGenes),
    is.character(proj_name)
  )
  
  params <- newSplatParams()
  
  params <- setParams(params, update = list(
    batchCells = batchCells, 
    group.prob = group.prob, 
    de.prob = de.prob,
    nGenes = nGenes
    ))
  
  sim <- splatSimulate(params, method = "groups")
  
  counts_matrix <- counts(sim)
  seurat_obj <- CreateSeuratObject(counts = counts_matrix, project = proj_name)
  
  # then we want to add the true labelling in case we want to do arandi
  seurat_obj$true_labelling <- colData(sim)$Group
  
  return(seurat_obj)
  
}


# seurat_object - on which we want to perform our clustering
preprocess <- function(seurat_object) {
  # takes seurat object and performs sctransform and pca steps
  
  stopifnot(
    (class(seurat_object) == "Seurat")
  )
  
  seurat_object <- SCTransform(seurat_object, verbose = FALSE)
  
  seurat_object <- RunPCA(seurat_object, verbose = FALSE)
  
  return(seurat_object)
  
}

# one thing we really need to check is if the pbmc is single cell - 99% sure it is but conscious I haven't actually checked. 

# now, we have seurat objects, sctransformed (normalised and scaled and high var features identified etc) and PCA'd, and we want to cluster them. The easiest to implement will be Louvain and then Leiden, as they are choices that actually make sense. slightly harder to implement will be kmeans, and slightly harder than that will be dbscan. We start with kmeans as nice middle ground. 

# in terms of time efficiency, make it exist first, then make it good. we can ask AI for all the timesaving tips about something that exists, but it's not useful if we're getting it to write the function in the first place. so we begin. 

# seurat_object - a processed seurat object
# npcs - number of pcs to consider
# k - numer of clusters
run_kmeans <- function(seurat_object, npcs, k) {
  # takes a processed seurat object, runs kmeans on it and returns an object of class kmeans
  
  stopifnot(
    (class(seurat_object) == "Seurat"), 
    is.numeric(npcs),
    is.numeric(k)
  )
  
  pc_embeddings <- Embeddings(seurat_object, reduction = "pca")[, 1:npcs]
  
  kmobject <- kmeans(pc_embeddings, centers = k, nstart = 25)
  
  return(kmobject)
  
}

```

```{r Simulated Dataset}

simulated1 <- seurat_from_params(
  batchCells = 500,
  group.prob = c(0.25, 0.25, 0.25, 0.25),
  de.prob = 0.2, 
  nGenes = 3000,
  proj_name = "simulated1"
)

```

```{r pbmc Dataset}

pbmc_data <- Read10X("C:/Users/fmhfl/Desktop/Undergrad Project/undergrad_project/DATA/filtered_gene_bc_matrices/hg19/")

pbmc <- CreateSeuratObject(counts = pbmc_data)

```


```{r Init Results Dataframe}

RESULTS <- tibble(
  run_name = character(),
  algorithm = character(),
  
  k = integer(),
  eps = numeric(),
  minpts = integer(),
  resolution = numeric(),
  
  dataset = character(),
  ncells = integer(),
  nfeatures = integer(),
  npcs = integer(),
  
  repindex = integer(),
  random_seed = numeric(),
  start_time = POSIXct(),
  runtime = numeric(),
  
  cluster_numer = integer(),
  cluster_sizes = list(),
  
  sil_avg = numeric(),
  sil_sd = numeric(),
  arandi = numeric()
)


```



```{r Scratch}

p_simulated1 <- preprocess(simulated1)

km_simulated1 <- run_kmeans(p_simulated1, 30, 4)

# ok so this seems to be working about as intended.
# since we're not going to have to generalise to dataset (we're only going to do 2 or 3) I'm ok with choosing k manually, for now at least. 
# now we get our index extraction.

```



