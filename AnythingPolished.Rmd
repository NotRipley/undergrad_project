---
title: "AnythingPolished"
output: html_document
---

Compilation of polished productive code from other rmd files, in a format that can be sent over as a progress update. 

### The Simple Simulation with KMeans

```{r Libraries}

library(tidyverse) # general
library(mclust) # for the adj rand index

```

```{r Functions}

# m - number of rows per cluster
# n - number of dimensions for the dataset (number of columns)
# N - noise dimensions to be added to the dataset 'in post' of the generation
# k - clusters to generate
# Overlap - boolean variable, whether the variances of each cluster should be inflated s.t. they overlap, or should they be well-separated?
# Equal-In - boolean - should my clusters be spherical, or 'ovoid'?
# Equal-Out - equal variance between clusters, or randomly allocate? LEAVE FOR NOW
simple_data <- function(m, n, N, k, overlap, equalin) {
  
  # make sure nothing's gonna break
  stopifnot(
    is.logical(overlap), 
    is.logical(equalin)
    #is.logical(equalout)
  )
  
  # check some options based on our arguments
  if (overlap == TRUE) {
    p_var <- rep(0.5, k) 
  } else {
    p_var <- rep(0.2, k)
  }
  
  if (equalin == FALSE) {
    p_var <- abs(matrix(rnorm(k, p_var[[1]], 0.25),  nrow = 1, ncol = k)) # might need slight adjusting
    
  }
  
  unmerged <- list()
  
  # create centre data as separate matrices in 'unmerged'
  for (i in 1:k) {
    cat("generating cluster ", i, "... \n")
    unmerged[[i]] <- matrix(rnorm(m*n, i, p_var[[i]]), nrow = m, ncol = n)
    
    noise <- matrix(rnorm(N*m, mean = 0, sd = 0.3), nrow = m, ncol = N)
    unmerged[[i]] <- cbind(unmerged[[i]], noise)
    
    unmerged[[i]] <- cbind(unmerged[[i]], matrix(data = as.numeric(i), nrow = m, ncol = 1)) # add tag column
    
    cat("finished cluster ", i, "... \n")
  }
  
  cat("finished all clusters... \n")
  
  frame <- as.data.frame(do.call(rbind, unmerged)) # smush the matrices together
  
  names(frame)[1:n] <- paste0("dim", 1:n)
  if (N != 0) {
    names(frame)[(n + 1):(n + N)] <- paste0("noise", 1:N)
  }
  
  names(frame)[ncol(frame)] <- "Tag"
  
  return(frame)
  
}

# data - the dataframe that is generated 
# k - the true number of centres. hence, this will only work on our artificial simple data
# ignore_last - logical representing whether to cluster on the final col (i.e. is it a tag col)
kmeans_true_k <- function(data, k, ignore_last) {
  
  # a sprinkle of input checking
  stopifnot(
    is.data.frame(data),
    is.numeric(k),
    is.logical(ignore_last)
  )
  
  # drop last col if it's a tag col
  if (ignore_last == TRUE) {
    p_frame <- data[, -ncol(data)]
  } else {
    p_frame <- data
  }
  
  # kmeans on true k. In future can roll the estimation into this function maybe?
  kmobject <- kmeans(p_frame, k, iter.max = 20, nstart = 25)
  
  return(kmobject)
  
}

```

```{r Simulation}

# output will have 800 rows, 10^4 + 1 columns, one of which will be a tag column
sim_data <- simple_data(200, 10, 10^6 - 10, 4, TRUE, TRUE)

# 10 output points, starting small so we check it works
log_sequence <- unique(round(10^seq(1, 4, length.out = 10)))

scores <- numeric(length(log_sequence))

for (i in seq_along(log_sequence)) {
  current_slice <- sim_data[, 1:log_sequence[i]]
  kmobject <- kmeans_true_k(current_slice, 4, FALSE) # we don't ignore last because we never pass it all 10011
  scores[i] <- mclust::adjustedRandIndex(sim_data$Tag, kmobject$cluster)
  
  cat("Dimension", log_sequence[i], "gives score: ", round(scores[i], 4), " \n")
}

```

```{r Plot}



```

What do we take away from this? So far, it's not working. The only thing I can think of is that I add noise inside the for loop, which may mean that it carries a bit of signal? That wouldn't make any sense though. 

### Comparison of KMeans vs DBScan on Simple Data

```{r Libraries2}

library(dbscan)
library(mlbench)

```


In this section, we are not demonstrating the curse of dimensionality with DBScan. We are going for a straight up performance comparison of KMeans to DBScan on simple data. Since we are generating the data ourselves, we can use a performance index that takes the ground truth, in our case Adjust Rand Index. 

We check on medium- and high-dimensional data, and with well-separated and overlapping clusters. 

DBScan will hasve serious challenges (from what I understand) as the dimensionality of the data exceeds our sample size. I think that this is down to the distance convergence as dimension increases; the ratio of distances to two points in n-space goes to 0 as n goes to infinity. This will almost always be the case in the splatter and real data. Hence, even now when we could artificially get around that, we implement our dimensionality reduction here. 

For now, just some PCA. We can check nonlinear tSNE and UMAP later. 

When we come back to this, we will finish this pca thing and check the dbscan performance on simple data. 

```{r Functions2}

simple_dbscan <- function(data, pcs, ignore_last) {
  
  # first step we reduce to a desired number of pcs
  
  if (ignore_last == TRUE) {
    p_data <- data[, -ncol(data)]
  } else {
    p_data <- data
  }
  
  pc_data <- prcomp(p_data, scale = TRUE)
  
  return(pc_data)
  
  
}

```

We get a first PC explaining the vast majority of the variation because of the way we have generated the data. It may be that it's not really meaningful to compare kmeans and dbscan on this same simulation. I think that we can easily come back and do that with mlbench if we want. But for now, I'll focus on getting the splatter sim data up. Then, I want to generate some realistic data with it, and go for some comparisons of the performance of kmeans with and without pca, and dbscan with and without pca. 

```{r Simulation2}

data1 <- simple_data(200, 1000, 0, 4, TRUE, FALSE)

pcs1 <- simple_dbscan(data1, 50, TRUE)

var_explained <- (pcs1$sdev^2 / sum(pcs1$sdev^2))

qplot(c(1:10), var_explained[1:10]) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("Variance Explained") +
  ggtitle("Scree Plot") +
  ylim(0, 1)

spirals <- mlbench.spirals(300, 1.5, 0.0)
cassini <- mlbench.cassini(300)


plot(spirals)
plot(cassini)

```


### Comparison of Cluster Validation Metrics



### Performance on Splatter Data

We create a function which loads in a new SplatParams object and sets the relevant parameters to a specified value. 
There's so many parameters, we should be careful not to overdo our function, adding in the ability to change from default values just as needed. To bear in mind: check the differential expression params. 

Anyway, it's good that we've started on this stuff. Big mountain but plodding. 

```{r Libraries3}

# Load package
suppressPackageStartupMessages({
    library(splatter)
    library(scater)
    library(Seurat)
})

```

```{r Functions3}

# batchCells - a vector giving the number of cells per batch, ie two batches with 100 cells is (100, 100)
# group.prob - vector specifying probabilities per group, ie (0.5, 0.5) will likely give two same sized groups
# de.prob - probability that genes will be DE by group(?) i.e. that they'll differ more than just noise
# nGenes - number of genes to simulate
# all else we leave as default values. 
seurat_from_params <- function(batchCells, group.prob, de.prob, nGenes, proj_name) {
  
  stopifnot(
    is.numeric(batchCells),
    is.numeric(group.prob),
    is.numeric(de.prob),
    is.numeric(nGenes),
    is.character(proj_name)
  )
  
  params <- newSplatParams()
  
  params <- setParams(params, update = list(
    batchCells = batchCells, 
    group.prob = group.prob, 
    de.prob = de.prob,
    nGenes = nGenes
    ))
  
  sim <- splatSimulate(params, method = "groups")
  
  counts_matrix <- counts(sim)
  seurat_obj <- CreateSeuratObject(counts = counts_matrix, project = proj_name)
  
  # then we want to add the true labelling in case we want to do arandi
  seurat_obj$true_labelling <- colData(sim)$Group
  
  return(seurat_obj)
  
}

```

We will do the standard pre-processing workflow from the seurat tutorial, on our simulated data. Hopefully it has what we need there. Then, we can compare the performance of dbscan and kmeans on it, using the ground truth that we still have in this case.


```{r Simulation3}

seurat1 <- seurat_from_params(
  batchCells = 500,
  group.prob = c(0.25, 0.25, 0.25, 0.25),
  de.prob = 0.2, 
  nGenes = 3000,
  proj_name = "seurat1"
)

# so with bulk RNA-seq, the processing workflow goes:
# - drop cells we don't want
# - normalise, scale, find variable features
# - linear dimred
# - cluster
# - nonlinear dimred
# whereas in scRNA-seq:
# - SCTransform
# - linear and nonlinear dimred, cluster, visualise

# store mitochondrial percentage in object meta data
seurat1 <- PercentageFeatureSet(seurat1, pattern = "^MT-", col.name = "percent.mt")

# run sctransform
seurat1 <- SCTransform(seurat1, vars.to.regress = "percent.mt", verbose = FALSE)

```
```{r Visualisation Steps}

# standard steps apparently

seurat1 <- RunPCA(seurat1, verbose = FALSE)
seurat1 <- RunUMAP(seurat1, dims = 1:30, verbose = FALSE)

seurat1 <- FindNeighbors(seurat1, dims = 1:30, verbose = FALSE)
seurat1 <- FindClusters(seurat1, verbose = FALSE)
DimPlot(seurat1)

```
```{r All Together Now}

seurat2 <- seurat_from_params(batchCells = 300, group.prob = c(0.3, 0.3, 0.4), de.prob = 0.1, nGenes = 3000, proj_name = "seurat2")

# store mitochondrial percentage in object meta data - it is important to realise that what this is doing is saying 'we want to control for healthy vs dying cells because having a really high amount of mitochondrial expression is a sign that something is wrong", i think, and so the vars to regress controls for that in downstream analysis. 
seurat2 <- PercentageFeatureSet(seurat2, pattern = "^MT-", col.name = "percent.mt")

# run sctransform
seurat2 <- SCTransform(seurat2, vars.to.regress = "percent.mt", verbose = FALSE)

seurat2 <- RunPCA(seurat2, verbose = FALSE)
seurat2 <- RunUMAP(seurat2, dims = 1:30, verbose = FALSE)

seurat2<- FindNeighbors(seurat2, dims = 1:30, verbose = FALSE)
seurat2 <- FindClusters(seurat2, verbose = FALSE)

DimPlot(seurat2)

```



### Dimensionality Reduction - PCA and tSNE



### Performance on Real Data


