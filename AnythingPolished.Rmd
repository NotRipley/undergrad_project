---
title: "AnythingPolished"
output: html_document
---

Compilation of polished productive code from other rmd files, in a format that can be sent over as a progress update. 

### The Simple Simulation with KMeans

```{r Libraries}

library(tidyverse) # general
library(mclust) # for the adj rand index

```

```{r Functions}

# m - number of rows per cluster
# n - number of dimensions for the dataset (number of columns)
# N - noise dimensions to be added to the dataset 'in post' of the generation
# k - clusters to generate
# Overlap - boolean variable, whether the variances of each cluster should be inflated s.t. they overlap, or should they be well-separated?
# Equal-In - boolean - should my clusters be spherical, or 'ovoid'?
# Equal-Out - equal variance between clusters, or randomly allocate? LEAVE FOR NOW
simple_data <- function(m, n, N, k, overlap, equalin) {
  
  # make sure nothing's gonna break
  stopifnot(
    is.logical(overlap), 
    is.logical(equalin)
    #is.logical(equalout)
  )
  
  # check some options based on our arguments
  if (overlap == TRUE) {
    p_var <- rep(0.5, k) 
  } else {
    p_var <- rep(0.2, k)
  }
  
  if (equalin == FALSE) {
    p_var <- abs(matrix(rnorm(k, p_var[[1]], 0.25),  nrow = 1, ncol = k)) # might need slight adjusting
    
  }
  
  unmerged <- list()
  
  # create centre data as separate matrices in 'unmerged'
  for (i in 1:k) {
    cat("generating cluster ", i, "... \n")
    unmerged[[i]] <- matrix(rnorm(m*n, i, p_var[[i]]), nrow = m, ncol = n)
    
    noise <- matrix(rnorm(N*m, mean = 0, sd = 0.3), nrow = m, ncol = N)
    unmerged[[i]] <- cbind(unmerged[[i]], noise)
    
    unmerged[[i]] <- cbind(unmerged[[i]], matrix(data = as.numeric(i), nrow = m, ncol = 1)) # add tag column
    
    cat("finished cluster ", i, "... \n")
  }
  
  cat("finished all clusters... \n")
  
  frame <- as.data.frame(do.call(rbind, unmerged)) # smush the matrices together
  
  names(frame)[1:n] <- paste0("dim", 1:n)
  if (N != 0) {
    names(frame)[(n + 1):(n + N)] <- paste0("noise", 1:N)
  }
  
  names(frame)[ncol(frame)] <- "Tag"
  
  return(frame)
  
}

# data - the dataframe that is generated 
# k - the true number of centres. hence, this will only work on our artificial simple data
# ignore_last - logical representing whether to cluster on the final col (i.e. is it a tag col)
kmeans_true_k <- function(data, k, ignore_last) {
  
  # a sprinkle of input checking
  stopifnot(
    is.data.frame(data),
    is.numeric(k),
    is.logical(ignore_last)
  )
  
  # drop last col if it's a tag col
  if (ignore_last == TRUE) {
    p_frame <- data[, -ncol(data)]
  } else {
    p_frame <- data
  }
  
  # kmeans on true k. In future can roll the estimation into this function maybe?
  kmobject <- kmeans(p_frame, k, iter.max = 20, nstart = 25)
  
  return(kmobject)
  
}

```

```{r Simulation}

# output will have 800 rows, 10^4 + 1 columns, one of which will be a tag column
sim_data <- simple_data(200, 10, 10^6 - 10, 4, TRUE, TRUE)

# 10 output points, starting small so we check it works
log_sequence <- unique(round(10^seq(1, 4, length.out = 10)))

scores <- numeric(length(log_sequence))

for (i in seq_along(log_sequence)) {
  current_slice <- sim_data[, 1:log_sequence[i]]
  kmobject <- kmeans_true_k(current_slice, 4, FALSE) # we don't ignore last because we never pass it all 10011
  scores[i] <- mclust::adjustedRandIndex(sim_data$Tag, kmobject$cluster)
  
  cat("Dimension", log_sequence[i], "gives score: ", round(scores[i], 4), " \n")
}

```

```{r Plot}



```

What do we take away from this? So far, it's not working. The only thing I can think of is that I add noise inside the for loop, which may mean that it carries a bit of signal? That wouldn't make any sense though. 

### Comparison of KMeans vs DBScan on Simple Data

```{r Libraries}

library(dbscan)

```


In this section, we are not demonstrating the curse of dimensionality with DBScan. We are going for a straight up performance comparison of KMeans to DBScan on simple data. Since we are generating the data ourselves, we can use a performance index that takes the ground truth, in our case Adjust Rand Index. 

We check on medium- and high-dimensional data, and with well-separated and overlapping clusters. 

DBScan will hasve serious challenges (from what I understand) as the dimensionality of the data exceeds our sample size. I think that this is down to the distance convergence as dimension increases; the ratio of distances to two points in n-space goes to 0 as n goes to infinity. This will almost always be the case in the splatter and real data. Hence, even now when we could artificially get around that, we implement our dimensionality reduction here. 

For now, just some PCA. We can check nonlinear tSNE and UMAP later. 

When we come back to this, we will finish this pca thing and check the dbscan performance on simple data. 

```{r Functions}

simple_dbscan <- function(data, pcs, ignore_last) {
  
  # first step we reduce to a desired number of pcs
  
  if (ignore_last == TRUE) {
    p_data <- data[, -ncol(data)]
  } else {
    p_data <- data
  }
  
  pc_data <- prcomp(p_data)
  
  # reverse sign?
  # pc_data$rotation <- -1*pc_data$rotation
  # pc_data$x <- -1*pc_data*x
  
  return(pc_data)
  
  
}

```

```{r Simulation}

data1 <- simple_data(200, 1000, 0, 4, FALSE, TRUE)

pcs1 <- simple_dbscan(data1, 50, TRUE)

head(pcs1$x)


```


### Comparison of Cluster Validation Metrics



### Dimensionality Reduction - PCA and tSNE



### Performance on Splatter Data

We create a function which loads in a new SplatParams object and sets the relevant parameters to a specified value. 
There's so many parameters, we should be careful not to overdo our function, adding in the ability to change from default values just as needed. To bear in mind: check the differential expression params. 

Anyway, it's good that we've started on this stuff. Big mountain but plodding. 

```{r Libraries}

# Load package
suppressPackageStartupMessages({
    library(splatter)
    library(scater)
    library(Seurat)
})

```

```{r Functions}

# batchCells - a vector giving the number of cells per batch, ie two batches with 100 cells is (100, 100)
# group.prob - vector specifying probabilities per group, ie (0.5, 0.5) will likely give two same sized groups
# de.prob - probability that genes will be DE by group(?) i.e. that they'll differ more than just noise
# nGenes - number of genes to simulate
# all else we leave as default values. 
seurat_from_params <- function(batchCells, group.prob, de.prob, nGenes) {
  
  params <- newSplatParams()
  
  params <- setParams(params, update = list(
    batchCells = batchCells, 
    group.prob = group.prob, 
    de.prob = de.prob,
    nGenes = nGenes
    ))
  
  sim <- splatSimulate(params, method = "groups")
  
  counts_matrix <- counts(sim)
  seurat_obj <- CreateSeuratObject(counts = counts_matrix)
  
  # then we want to add the true labelling in case we want to do arandi
  seurat_obj$true_labelling <- colData(sim)$Group
  
  return(seurat_obj)
  
}

```

```{r Simulate}

seurat1 <- seurat_from_params(
  batchCells = 500,
  group.prob = c(0.25, 0.25, 0.25, 0.25),
  de.prob = 0.2, 
  nGenes = 3000
)

seurat1

# ok looks like we have a data creation function here. We switch to 20 mins of looking for jobs, then come back and attack the dbscan bit. Then continue with this. Then soon hope to write this up in the methodology.

```


### Performance on Real Data


